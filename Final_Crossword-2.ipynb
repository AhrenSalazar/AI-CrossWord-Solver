{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpg8SNE2jxs8"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8f5IHiGxkpm",
        "outputId": "9d23adb7-0083-46f7-8e85-155d427515d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'11 19 Class.ipynb'\t    Final_Blah_Working_Decoder.ipynb\n",
            " answer_vectorizer.pkl\t    Final_Crossword.ipynb\n",
            "'Binomial Code.ipynb'\t   'Final_Crossword_UI_FIXED_SAFE (1).ipynb'\n",
            " CACA.ipynb\t\t   'Final Draft.ipynb'\n",
            "'CH10 document.ipynb.txt'  'HW 5'\n",
            "'Class 11 14 DV.ipynb'\t    nyt_crossword_colab_drive_upload.ipynb\n",
            " clue_vectorizer.pkl\t    nyt_crossword_generator_fixed.ipynb\n",
            " crossword_model.h5\t    nytcrosswords.csv\n",
            " crossword_model.keras\t   'NYT Solver.ipynb'\n",
            " Final_Blah_FIXED.ipynb    'Try 2.ipynb'\n",
            "'Final Blah.ipynb'\n"
          ]
        }
      ],
      "source": [
        "#Get cvs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# path for cvs\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/nytcrosswords.csv'\n",
        "import pandas as pd\n",
        "df = pd.read_csv(csv_path, encoding='latin1')\n",
        "!ls \"/content/drive/MyDrive/Colab Notebooks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Cc2zDdvbxkpn"
      },
      "outputs": [],
      "source": [
        "# Preprocess and split\n",
        "df = df.dropna(subset=[\"Word\"])\n",
        "df[\"input_text\"] = df[\"Clue\"].str.lower().str.strip()\n",
        "df[\"target_text\"] = df[\"Word\"].str.upper().str.strip()\n",
        "df = df[[\"input_text\", \"target_text\"]]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "oINVY_b0xkpo"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "clue_vectorizer = TextVectorization(output_mode='int', output_sequence_length=20)\n",
        "clue_vectorizer.adapt(train_df['input_text'])\n",
        "\n",
        "answer_vectorizer = TextVectorization(output_mode='int', output_sequence_length=16, split='character')\n",
        "answer_vectorizer.adapt(train_df['target_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Nke-yB1Dxkpo"
      },
      "outputs": [],
      "source": [
        "# Encoder-decoder LSTM model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "clue_vocab_size = len(clue_vectorizer.get_vocabulary())\n",
        "answer_vocab_size = len(answer_vectorizer.get_vocabulary())\n",
        "\n",
        "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
        "x = Embedding(clue_vocab_size, 128)(encoder_inputs)\n",
        "_, state_h, state_c = LSTM(256, return_state=True)(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
        "y = Embedding(answer_vocab_size, 128)(decoder_inputs)\n",
        "y, _, _ = LSTM(256, return_sequences=True, return_state=True)(y, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(answer_vocab_size, activation='softmax')(y)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hAOfGefVxkpp"
      },
      "outputs": [],
      "source": [
        "# Format data\n",
        "def format_dataset(df):\n",
        "    clues = tf.convert_to_tensor(df['input_text'].values)\n",
        "    answers = tf.convert_to_tensor(df['target_text'].values)\n",
        "    encoder_input = clue_vectorizer(clues)\n",
        "    decoder_input = answer_vectorizer(answers)[:, :-1]\n",
        "    decoder_target = answer_vectorizer(answers)[:, 1:]\n",
        "    return tf.data.Dataset.from_tensor_slices(((encoder_input, decoder_input), decoder_target)).shuffle(1024).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = format_dataset(train_df)\n",
        "val_ds = format_dataset(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP0XVOmBxkpp",
        "outputId": "7f99809a-22aa-4839-ecde-cb144065bf48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m10991/10991\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4202s\u001b[0m 382ms/step - accuracy: 0.7918 - loss: 0.6654 - val_accuracy: 0.8276 - val_loss: 0.5523\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=10) #Run '1' for test (Makes generator really really bad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pXvtBXTVkW7A"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/Colab Notebooks/crossword_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_kYNJ7znCMvH"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/clue_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(clue_vectorizer, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/answer_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(answer_vectorizer, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "KOoEQCKMD6al"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/crossword_model.keras')\n",
        "\n",
        "# Load vectorizers\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/clue_vectorizer.pkl', 'rb') as f:\n",
        "    clue_vectorizer = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/answer_vectorizer.pkl', 'rb') as f:\n",
        "    answer_vectorizer = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "1Wz2JNgdjwNe",
        "outputId": "19854e39-d8e8-435e-c87e-d2f1adc62a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ac86d39208e965575e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ac86d39208e965575e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "\n",
        "global decoder_lstm_infer\n",
        "decoder_lstm = decoder_lstm_infer\n",
        "\n",
        "# Lookup tables\n",
        "answer_vocab = answer_vectorizer.get_vocabulary()\n",
        "answer_index_lookup = {i: c for i, c in enumerate(answer_vocab)}\n",
        "answer_token_lookup = {c: i for i, c in enumerate(answer_vocab)}\n",
        "\n",
        "\n",
        "\n",
        "encoder_inputs = Input(shape=(None,), dtype='int32')\n",
        "embedding_layer = model.get_layer('embedding_15')\n",
        "lstm_layer = model.get_layer('lstm_15')  # Encoder LSTM\n",
        "x = embedding_layer(encoder_inputs)\n",
        "lstm_output = lstm_layer(x)\n",
        "if isinstance(lstm_output, (list, tuple)):\n",
        "    lstm_output = lstm_output[0]\n",
        "state_h = state_c = lstm_output\n",
        "encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
        "\n",
        "# Create an inference-time LSTM layer and copy weights\n",
        "decoder_lstm_trained = model.get_layer(\"lstm_15\")\n",
        "\n",
        "# Create a new layer\n",
        "decoder_lstm_infer = LSTM(256, return_sequences=True, return_state=True, name=\"lstm_3_infer\")\n",
        "_ = decoder_lstm_infer(tf.zeros((1, 1, 128)))  # batch, sequence, embedding dim\n",
        "decoder_lstm_infer.set_weights(decoder_lstm_trained.get_weights())\n",
        "\n",
        "\n",
        "def decode_sequence(clue_text):\n",
        "    try:\n",
        "        # Vectorize the clue\n",
        "        clue_vec = clue_vectorizer([clue_text])\n",
        "\n",
        "        # Run the encoder\n",
        "        encoder_inputs = Input(shape=(None,), dtype='int32')\n",
        "        embedding_layer = model.get_layer('embedding_14')   #Mathc with mcurrent modle\n",
        "        lstm_layer = model.get_layer('lstm_14')\n",
        "        x = embedding_layer(encoder_inputs)\n",
        "        lstm_output = lstm_layer(x)\n",
        "        if isinstance(lstm_output, (list, tuple)):\n",
        "            lstm_output = lstm_output[0]\n",
        "        state_h = state_c = lstm_output\n",
        "        encoder_model = tf.keras.Model(encoder_inputs, [state_h, state_c])\n",
        "\n",
        "        state_h, state_c = encoder_model.predict(clue_vec)\n",
        "\n",
        "        # Lookup vocab\n",
        "        answer_vocab = answer_vectorizer.get_vocabulary()\n",
        "        answer_index_lookup = {i: str(c) for i, c in enumerate(answer_vocab)}\n",
        "        answer_token_lookup = {str(c): i for i, c in enumerate(answer_vocab)}\n",
        "\n",
        "        # Start decoding\n",
        "        start_token_id = answer_token_lookup.get('e', 2)\n",
        "        target_seq = tf.constant([[start_token_id]])\n",
        "\n",
        "        decoded_chars = []\n",
        "        decoder_embedding = model.get_layer('embedding_15')\n",
        "        decoder_lstm = model.get_layer('lstm_15')\n",
        "        decoder_dense = model.get_layer('dense_7')\n",
        "\n",
        "        for _ in range(16):\n",
        "            embedded = decoder_embedding(target_seq)\n",
        "            output, h, c = decoder_lstm(embedded, initial_state=[state_h, state_c])\n",
        "            logits = decoder_dense(output)\n",
        "            token_index = tf.argmax(logits[0, -1, :]).numpy()\n",
        "\n",
        "            #prevent out-of-bounds index\n",
        "            if token_index >= len(answer_vocab):\n",
        "                print(\"Token out of range:\", token_index)\n",
        "                token_index = 1  # [UNK]\n",
        "\n",
        "            token_char = answer_index_lookup.get(token_index, '')\n",
        "            if token_char == '' or token_char == '[END]':\n",
        "                break\n",
        "\n",
        "            decoded_chars.append(token_char)\n",
        "            target_seq = tf.constant([[token_index]])\n",
        "            state_h, state_c = h, c\n",
        "\n",
        "        return ''.join(decoded_chars)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error in decode_sequence:\", e)\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "#Gradio UI\n",
        "interface = gr.Interface(\n",
        "    fn=decode_sequence,\n",
        "    inputs=gr.Textbox(label='Crossword Clue'),\n",
        "    outputs=gr.Textbox(label='Predicted Answer'),\n",
        "    title='Crossword Clue Answer Generator'\n",
        ")\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
